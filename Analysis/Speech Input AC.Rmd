---
title: "Speech Input Triad"
author: ""
date: "August 28, 2019"
output: 
   html_document: 
     dev: png
     fig_height: 5
     fig_width: 5.5
     number_sections: yes
     toc: yes
     toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ARTool)
library(reshape2)
library(ez)
library(apa)
library(gridExtra)
library(phia)
library(viridis)
library(lsmeans)
library(gmodels)
library(DescTools)
library(MASS)
library(pander)
library(reticulate)
library(ggpubr)
library(xtable)
# Needed to correctly export fonts in pdf (may not be required)
library(extrafont)
# Need to call extrafont::font_import() once in console and restart RStudio
```

```{r functions, echo=FALSE, warning=FALSE}

normalCheck = function(model) {
    res = residuals(model)
    qqnorm((res - mean(res)) / sd(res))
    abline(0, 1)
    print (shapiro.test(res))
}

createPlot = function(data, mean_var, cl_var, cu_var, vj, lx, ly, y_axis, lim_min = 0, lim_max = 33, breaks = 5, x_var = "taskType", angle = 0) {
  localenv <- environment()
  if (is.element(mean_var, c("mIT", "mTT", "mET", "mPT", "mCT"))) {
    data[mean_var] = data[mean_var] / 1000
    data[cl_var] = data[cl_var] / 1000
    data[cu_var] = data[cu_var] / 1000
  } 
  
  if (mean_var=="m_cer" | mean_var=="m_uer") {
    data[mean_var] = data[mean_var] * 100
    data[cl_var] = data[cl_var] * 100
    data[cu_var] = data[cu_var] * 100
  }
  
  ggplot(data, aes(x=data[[x_var]], y=data[[mean_var]], fill=inputType), environment = localenv ) +
  geom_bar(aes(group=inputType), position = position_dodge(.7), colour="black", stat="identity", width=.7) +
  geom_errorbar(aes(ymin = data[[cl_var]], ymax = data[[cu_var]]), width = 0.2, size = .7, position = position_dodge(.7)) +
  #geom_text(aes(label=round(data[[mean_var]],digits=2)), size = 4, position = position_dodge(0.7), vjust = vj, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name=y_axis, limits = c(lim_min, lim_max), minor_breaks = breaks, expand = c(0, 0)) +
  #scale_fill_viridis(option="inferno", begin=0.5, end = .8, discrete = TRUE) +
  #scale_fill_viridis(option="magma", begin=0.5, end = .8, discrete = TRUE) +
  scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "Input Type") +
  theme(legend.position = c(lx, ly),
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="10"),
        axis.text = element_text(family="Helvetica", colour="black", size="10"),
        axis.text.x=element_text(angle=angle, hjust=1),
        panel.grid.major.y = element_line( size = .1, color = "grey"))
}
```

```{python usefulfunctions, echo=FALSE}
import re
def parseAnova(s):
  aovres = re.findall(r'[-+]?\d*\.\d+|\d+',s)
  return "\\anova{%s}{%s}{%s}{%s}{%s}"%(aovres[0], aovres[1], aovres[2], aovres[3], aovres[4])
  
def writeToFile(s, file):
  f = open("autogen/" + file,"w+")
  f.write("% do not edit this file as it was automatically generated\n\n")
  f.write(s)
  f.close()
```

# Data Parsing

## Loading Data

Filtering out invalid participants (9, 10, and 14).

```{r load}
data_c = read.csv("parse_more.csv",sep=",") %>% filter(!participantNo %in% c(9, 10, 14))
data_c = data_c[,!names(data_c) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "correctMsg", "gMsg")]
data_c.kbd = filter(data_c, inputType=="Keyboard")
data_c.sr = filter(data_c, inputType=="Speech")

data_t = read.csv("parse_more_trans.csv",sep=",")
data_t = data_t[,!names(data_t) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "orgMsg")]
data_t$setNum = substr(data_t$setNum, 1, 1)
data_t.kbd = filter(data_t, inputType=="Keyboard")
data_t.sr = filter(data_t, inputType=="Speech")

```

## Outlier Culling

I removed values outside of 3 standard deviations for the means of each task x method combination, for totalTime.

22 outliers identified for composition (2.04% of trials)

```{r outliers_comp, warning=FALSE, message=FALSE, echo=FALSE}
dcs = summarize(group_by(data_c, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

cko.tt <- filter(data_c.kbd, totalTime < (dcs$mTT[1] - 3 * dcs$sdTT[1]) | totalTime > (dcs$mTT[1] + 3 * dcs$sdTT[1]))
cso.tt <- filter(data_c.sr, totalTime < (dcs$mTT[2] - 3 * dcs$sdTT[2]) | totalTime > (dcs$mTT[2] + 3 * dcs$sdTT[2]))
co.tt <- bind_rows(cko.tt, cso.tt)

data_c.clean = anti_join(data_c, co.tt)
#remove(co.all, co.it, co.pt, co.et, co.tt, co.wpm, co.uer, co.cer)
```

26 outliers identified for transcription (2.32% of trials)

```{r outlier_trans, warning=FALSE, message=FALSE, echo=FALSE}
dts = summarize(group_by(data_t, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

tko.tt <- filter(data_t.kbd, totalTime < (dts$mTT[1] - 3 * dts$sdTT[1]) | totalTime > (dts$mTT[1] + 3 * dts$sdTT[1]))
tso.tt <- filter(data_t.sr, totalTime < (dts$mTT[2] - 3 * dts$sdTT[2]) | totalTime > (dts$mTT[2] + 3 * dts$sdTT[2]))
to.tt <- bind_rows(tko.tt, tso.tt)
# 
# to.all <- bind_rows(to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)
# to <- unique(to.all)

data_t.clean = anti_join(data_t, to.tt)
#remove(to.all, to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)

```

## Data Aggregation

Aggregating the cleaned data

```{r warning=FALSE}
data = bind_rows(data_t.clean, data_c.clean)
data$acUsed <- ifelse(data$acCount > 1, TRUE, FALSE)

data.group = summarize(group_by(filter(data, inputType == "Keyboard"), participantNo, inputType, taskType, acUsed), meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.stats = data %>% filter(inputType == "Keyboard") %>%
  group_by(inputType, taskType, acUsed) %>%
  summarize(mIT = mean(inputTime), cuIT = ci(inputTime)[3], clIT = ci(inputTime)[2], sdIT = sd(inputTime),
            mPT = mean(prepTime), cuPT = ci(prepTime)[3], clPT = ci(prepTime)[2], sdPT = sd(prepTime),
            mET = mean(entryTime), cuET=ci(entryTime)[3], clET = ci(entryTime)[2], sdET = sd(entryTime),
            mTT = mean(totalTime), cuTT = ci(totalTime)[3], clTT = ci(totalTime)[2], sdTT = sd(totalTime),
            mCT = mean(corrTime), cuCT = ci(corrTime)[3], clCT = ci(corrTime)[2], sdCT = sd(corrTime),
            mWpm = mean(wpm), cuWpm = ci(wpm)[3], clWpm = ci(wpm)[2], sdWpm = sd(wpm),
            m_uer = mean(uncorrected_error_rate), cu_uer = ci(uncorrected_error_rate)[3], cl_uer = ci(uncorrected_error_rate)[2], sd_uer = sd(uncorrected_error_rate),
            m_cer= mean(corrected_error_rate), cu_cer = ci(corrected_error_rate)[3], cl_cer = ci(corrected_error_rate)[2], sd_cer = sd(corrected_error_rate),
            m_bd = mean(bandwidth), cuBD = ci(bandwidth)[3], clBD = ci(bandwidth)[2], sdBD = sd(bandwidth))

data.long = melt(data.group, id = c("participantNo", "inputType", "taskType", "acUsed", "meanInputTime", "meanPrepTime", "meanWpm", "meanTotalTime", "meanEntryTime", "meanLen", "mean_uer", "mean_cer", "mean_bd", "meanCorrTime"))
#
data.long$participantNo <- factor(data.long$participantNo)
data.long$inputType <- factor(data.long$inputType)
data.long$taskType <- factor(data.long$taskType)
data.long$acUsed <- factor(data.long$acUsed)
```

# Autocorrect Analysis

```{r}
t_kbd <- filter(data_t.clean, inputType == "Keyboard")
nrow(t_kbd)
c_kbd <- filter(data_c.clean, inputType == "Keyboard")
nrow(c_kbd)
sum(t_kbd$acCount > 1)/nrow(t_kbd)
sum(c_kbd$acCount > 1)/nrow(c_kbd)
```

```{r warning=FALSE, message=FALSE}
ggplot(data.stats, aes(x=taskType, y=mWpm, fill=acUsed)) + 
  geom_bar(aes(group=acUsed), position = position_dodge(.7), colour="black", stat="identity", width=.7) + 
  geom_errorbar(aes(ymin = clWpm, ymax = cuWpm), width = 0.2, size = .5, position = position_dodge(.7)) +
  geom_text(aes(label=round(mWpm,digits=2)), size = 4, position = position_dodge(0.7), vjust = 2.5, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name="Average WPM", limits = c(0, 70), minor_breaks = 5, expand = c(0, 0)) +
  scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "AC Used?") +
  theme(legend.position = c(.15, .85),
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        axis.text = element_text(family="Helvetica", colour="black", size="14"),
        panel.grid.major.y = element_line( size = .1, color = "grey"),
        legend.title = element_text(family="Helvetica", face="bold", colour="black", size="12"),
        legend.text = element_text(family="Helvetica", face="bold", colour="black", size="12"))

ggplot(data.stats, aes(x=taskType, y=mIT/1000, fill=acUsed)) + 
  geom_bar(aes(group=acUsed), position = position_dodge(.7), colour="black", stat="identity", width=.7) + 
  geom_errorbar(aes(ymin = clIT/1000, ymax = cuIT/1000), width = 0.2, size = .5, position = position_dodge(.7)) +
  geom_text(aes(label=round(mWpm,digits=2)), size = 4, position = position_dodge(0.7), vjust = 2.5, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name="Average Input Time", limits = c(0, 70), minor_breaks = 5, expand = c(0, 0)) +
  scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "AC Used?") +
  theme(legend.position = c(.15, .85),
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        axis.text = element_text(family="Helvetica", colour="black", size="14"),
        panel.grid.major.y = element_line( size = .1, color = "grey"),
        legend.title = element_text(family="Helvetica", face="bold", colour="black", size="12"),
        legend.text = element_text(family="Helvetica", face="bold", colour="black", size="12"))
```


## Effects of Input Type

### Normality Check

```{r wpmnormal}
m <- aov(meanWpm ~ acUsed, data=data.long)
pander(normalCheck(m))
```

The data is normally distributed.

### Repeated measures ANOVA on transformed data

```{r wpmanova, warning=FALSE}
anova = ezANOVA(data.long, dv=.(meanWpm), wid=.(participantNo), within=.(acUsed), detailed=TRUE)

anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
task = anova_apa$text[3]
inter = anova_apa$text[4]
kable(anova_apa)
```

### Post-hoc comparisons
```{r}
t <- TukeyHSD(m)
kable(t$`acUsed`)
```

```