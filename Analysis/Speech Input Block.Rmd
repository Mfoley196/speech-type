---
title: "Speech Input Block"
author: ""
date: "August 13, 2019"
output: 
   html_document: 
     dev: png
     fig_height: 5
     fig_width: 5.5
     number_sections: yes
     toc: yes
     toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ARTool)
library(reshape2)
library(ez)
library(apa)
library(gridExtra)
library(phia)
library(viridis)
library(lsmeans)
library(gmodels)
library(DescTools)
library(MASS)
library(pander)
library(reticulate)
library(ggpubr)
library(xtable)
# Needed to correctly export fonts in pdf (may not be required)
library(extrafont)
# Need to call extrafont::font_import() once in console and restart RStudio
```

```{r functions, echo=FALSE, warning=FALSE}

normalCheck = function(model) {
    res = residuals(model)
    qqnorm((res - mean(res)) / sd(res))
    abline(0, 1)
    print (shapiro.test(res))
}

createPlot = function(data, mean_var, cl_var, cu_var, vj, lx, ly, y_axis, lim_min = 0, lim_max = 33, breaks = 5, x_var = "taskType", angle = 0) {
  localenv <- environment()
  if (is.element(mean_var, c("mIT", "mTT", "mET", "mPT", "mCT"))) {
    data[mean_var] = data[mean_var] / 1000
    data[cl_var] = data[cl_var] / 1000
    data[cu_var] = data[cu_var] / 1000
  } 
  
  if (mean_var=="m_cer" | mean_var=="m_uer") {
    data[mean_var] = data[mean_var] * 100
    data[cl_var] = data[cl_var] * 100
    data[cu_var] = data[cu_var] * 100
  }
  
  ggplot(data, aes(x=data[[x_var]], y=data[[mean_var]], fill=inputType), environment = localenv ) +
  geom_bar(aes(group=inputType), position = position_dodge(.7), colour="black", stat="identity", width=.7) +
  geom_errorbar(aes(ymin = data[[cl_var]], ymax = data[[cu_var]]), width = 0.2, size = .7, position = position_dodge(.7)) +
  geom_text(aes(label=round(data[[mean_var]],digits=2)), size = 4, position = position_dodge(0.7), vjust = vj, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name=y_axis, limits = c(lim_min, lim_max), minor_breaks = breaks, expand = c(0, 0)) +
  #scale_fill_viridis(option="inferno", begin=0.5, end = .8, discrete = TRUE) +
  #scale_fill_viridis(option="magma", begin=0.5, end = .8, discrete = TRUE) +
  scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "Input Type") +
  theme(legend.position = c(lx, ly),
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="10"),
        axis.text = element_text(family="Helvetica", colour="black", size="10"),
        axis.text.x=element_text(angle=angle, hjust=1),
        panel.grid.major.y = element_line( size = .1, color = "grey"))
}
```

```{python usefulfunctions, echo=FALSE}
import re
def parseAnova(s):
  aovres = re.findall(r'[-+]?\d*\.\d+|\d+',s)
  return "\\anova{%s}{%s}{%s}{%s}{%s}"%(aovres[0], aovres[1], aovres[2], aovres[3], aovres[4])
  
def writeToFile(s, file):
  f = open("autogen/" + file,"w+")
  f.write("% do not edit this file as it was automatically generated\n\n")
  f.write(s)
  f.close()
```

# Data Parsing

## Loading Data

Filtering out invalid participants (9, 10, and 14).

```{r load}
data_c = read.csv("parse_more.csv",sep=",") %>% filter(!participantNo %in% c(9, 10, 14))
data_c = data_c[,!names(data_c) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "correctMsg", "gMsg")]
data_c$totalTime = data_c$inputTime + data_c$prepTime
data_c.kbd = filter(data_c, inputType=="Keyboard")
data_c.sr = filter(data_c, inputType=="Speech")

data_t = read.csv("parse_more_trans.csv",sep=",")
data_t = data_t[,!names(data_t) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "orgMsg")]
data_t$totalTime = data_t$inputTime + data_t$prepTime
data_t$setNum = substr(data_t$setNum, 1, 1)
data_t.kbd = filter(data_t, inputType=="Keyboard")
data_t.sr = filter(data_t, inputType=="Speech")

```

## Outlier Culling

I removed values outside of 3 standard deviations for the means of each task x method combination, for totalTime.

22 outliers identified for composition (2.04% of trials)

```{r outliers_comp, warning=FALSE, message=FALSE, echo=FALSE}
dcs = summarize(group_by(data_c, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

cko.tt <- filter(data_c.kbd, totalTime < (dcs$mTT[1] - 3 * dcs$sdTT[1]) | totalTime > (dcs$mTT[1] + 3 * dcs$sdTT[1]))
cso.tt <- filter(data_c.sr, totalTime < (dcs$mTT[2] - 3 * dcs$sdTT[2]) | totalTime > (dcs$mTT[2] + 3 * dcs$sdTT[2]))
co.tt <- bind_rows(cko.tt, cso.tt)

data_c.clean = anti_join(data_c, co.tt)
#remove(co.all, co.it, co.pt, co.et, co.tt, co.wpm, co.uer, co.cer)
```

26 outliers identified for transcription (2.32% of trials)

```{r outlier_trans, warning=FALSE, message=FALSE, echo=FALSE}
dts = summarize(group_by(data_t, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

tko.tt <- filter(data_t.kbd, totalTime < (dts$mTT[1] - 3 * dts$sdTT[1]) | totalTime > (dts$mTT[1] + 3 * dts$sdTT[1]))
tso.tt <- filter(data_t.sr, totalTime < (dts$mTT[2] - 3 * dts$sdTT[2]) | totalTime > (dts$mTT[2] + 3 * dts$sdTT[2]))
to.tt <- bind_rows(tko.tt, tso.tt)
# 
# to.all <- bind_rows(to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)
# to <- unique(to.all)

data_t.clean = anti_join(data_t, to.tt)
#remove(to.all, to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)

```

## Data Aggregation

Aggregating the cleaned data

```{r warning=FALSE}
data = bind_rows(data_t.clean, data_c.clean)
data$block = cut(data$trialNo, breaks = c(0, 5, 10, 15, 21), 
                        labels = c(1, 2, 3, 4))
#c(0, 5, 10, 15, 21)
data.group = summarize(group_by(data, participantNo, inputType, taskType, block), meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.stats = data %>%
  group_by(block) %>%
  summarize(mIT = mean(inputTime), cuIT = ci(inputTime)[3], clIT = ci(inputTime)[2], sdIT = sd(inputTime),
            mPT = mean(prepTime), cuPT = ci(prepTime)[3], clPT = ci(prepTime)[2], sdPT = sd(prepTime),
            mET = mean(entryTime), cuET=ci(entryTime)[3], clET = ci(entryTime)[2], sdET = sd(entryTime),
            mTT = mean(totalTime), cuTT = ci(totalTime)[3], clTT = ci(totalTime)[2], sdTT = sd(totalTime),
            mCT = mean(corrTime), cuCT = ci(corrTime)[3], clCT = ci(corrTime)[2], sdCT = sd(corrTime),
            mWpm = mean(wpm), cuWpm = ci(wpm)[3], clWpm = ci(wpm)[2], sdWpm = sd(wpm),
            m_uer = mean(uncorrected_error_rate), cu_uer = ci(uncorrected_error_rate)[3], cl_uer = ci(uncorrected_error_rate)[2], sd_uer = sd(uncorrected_error_rate),
            m_cer= mean(corrected_error_rate), cu_cer = ci(corrected_error_rate)[3], cl_cer = ci(corrected_error_rate)[2], sd_cer = sd(corrected_error_rate),
            m_bd = mean(bandwidth), cuBD = ci(bandwidth)[3], clBD = ci(bandwidth)[2], sdBD = sd(bandwidth))

data.long = melt(data.group, id = c("participantNo", "inputType", "taskType", "block", "meanInputTime", "meanPrepTime", "meanWpm", "meanTotalTime", "meanEntryTime", "meanLen", "mean_uer", "mean_cer", "mean_bd", "meanCorrTime"))

data.long$participantNo <- factor(data.long$participantNo)
data.long$inputType <- factor(data.long$inputType)

data.long.c = filter(data.long, taskType == "Composition")
data.long.c = data.long.c[,!names(data.long.c) %in% c("taskType")]

data.long.t = filter(data.long, taskType == "Transcription")
data.long.t = data.long.t[,!names(data.long.t) %in% c("taskType")]

#data.long$setNum <- factor(data.long$setNum)
data.long$block <- factor(data.long$block)
data.long$taskType <- factor(data.long$taskType)
# 
# data.long.c$setNum <- factor(data.long.c$setNum)
# data.long.t$setNum <- factor(data.long.t$setNum)
```

Output data stats to a Latex table

Psych, this didn't actually work that well....
```{r}
#data.stats.i = unite(data.stats, "ti", taskType, inputType) 
#data.stats.i = data.stats.i[order(data.stats.i$ti),]

#data.tbl <- dplyr::select(data.stats.i, ti, mTT, sdTT, mIT, sdIT, mPT, sdPT, mET, sdET, mWpm, sdWpm, m_uer, sd_uer, m_cer, sd_cer) %>% t %>% as.data.frame()

#print(xtable(data.tbl, type = "latex"), file = "autogen/table.tex")
```

# Input Time

## Effects of Input Type, Task Type, and Block

### Normality Check

```{r itnormal}
m <- aov(meanInputTime ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation
```{r itbc}
boxcox(meanInputTime ~ inputType*taskType*block, data=data.long, plotit=T)
```

An effective lambda value is 0.0 (a.k.a, a log transformation).

```{r ittransform}
datatr = data.long %>%
    mutate(meanInputTime = log(meanInputTime))

m <- aov(meanInputTime ~ taskType*inputType*block, data=datatr)
pander(normalCheck(m))
```

### Repeated measures ANOVA on transformed data

```{r itanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanInputTime), wid=.(participantNo), within=.(inputType,taskType,block), detailed=TRUE)
# kable(anova$`Mauchly's Test for Sphericity`)
# kable(anova$`Sphericity Corrections`)
# kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
task = anova_apa$text[3]
kable(anova_apa)
```

No effect of block, or the interaction of block with taskType and inputType.

### Post-hoc analysis with Tukey HSD

```{r itph}
# attach(datatr)
# pw <- pairwise.t.test(meanInputTime, interaction(inputType, taskType), p.adj = "bonferroni")
# detach(datatr)
# kable(pw$p.value)
```


# Prep Time

## Chart
```{r message=FALSE, warning=FALSE}
ggplot(data.stats, aes(x=block, y=mPT/1000, fill=block)) + 
  geom_bar(aes(group=block), position = position_dodge(.8), colour="black", stat="identity", width=.8) + 
  geom_errorbar(aes(ymin = clPT/1000, ymax = cuPT/1000, group= block), width = 0.2, size = .5, position = position_dodge(.8)) +
  geom_text(aes(label=round(mPT/1000,digits=2), group= block), size = 5, position = position_dodge(.8), vjust = 3, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name="Average Prep Time (s)", limits = c(0, 10), minor_breaks = 5, expand = c(0, 0)) +
  scale_fill_viridis(option="inferno", begin=0.5, end = .8, discrete = TRUE) +
  #scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "Input Type") +
  theme(legend.position = "bottom",
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        axis.text = element_text(family="Helvetica", colour="black", size="14"),
        panel.grid.major.y = element_line( size = .1, color = "grey"),
        legend.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        legend.text = element_text(family="Helvetica", face="bold", colour="black", size="14"))
```

## Effects of Input Type, Task Type, and Block

### Normality Check

```{r ptnormal}
m <- aov(meanPrepTime ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation
```{r ptbc}
boxcox(meanPrepTime ~ inputType*taskType*block, data=data.long, plotit=T)
```

An effective lambda value is -0.1.

```{r pttransform}
datatr = data.long %>%
    mutate(meanPrepTime = meanPrepTime^(-0.1))

m <- aov(meanPrepTime ~ taskType*inputType*block, data=datatr)
pander(normalCheck(m))

```

### Repeated measures ANOVA on transformed data

```{r ptanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanPrepTime), wid=.(participantNo), within=.(inputType,taskType,block), detailed=TRUE)
# kable(anova$`Mauchly's Test for Sphericity`)
# kable(anova$`Sphericity Corrections`)
# kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
task = anova_apa$text[3]
kable(anova_apa)
```

Block has an effect, but doesn't have any interactions with taskType or inputType.

### Post-hoc analysis with Tukey HSD

```{r ptph}
t <- TukeyHSD(m)
kable(t$`block`)
plot(t)
```

# Total Time

## Chart
```{r message=FALSE, warning=FALSE}
ggplot(data.stats, aes(x=block, y=mTT/1000, fill=block)) + 
  geom_bar(aes(group=block), position = position_dodge(.8), colour="black", stat="identity", width=.8) + 
  geom_errorbar(aes(ymin = clTT/1000, ymax = cuTT/1000, group= block), width = 0.2, size = .5, position = position_dodge(.8)) +
  geom_text(aes(label=round(mTT/1000,digits=2), group= block), size = 3.5, position = position_dodge(.8), vjust = 3, alignment="center") +
  scale_x_discrete(name="Task") +
  scale_y_continuous(name="Average Total Time (s)", limits = c(0, 37), minor_breaks = 5, expand = c(0, 0)) +
  scale_fill_viridis(option="inferno", begin=0.5, end = .8, discrete = TRUE) +
  #scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "Input Type") +
  theme(legend.position = "bottom",
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        axis.text = element_text(family="Helvetica", colour="black", size="14"),
        panel.grid.major.y = element_line( size = .1, color = "grey"),
        legend.title = element_text(family="Helvetica", face="bold", colour="black", size="14"),
        legend.text = element_text(family="Helvetica", face="bold", colour="black", size="14"))
```

## Effects of Input Type, Task Type and Block

### Normality Check

```{r ttnormal}
m <- aov(meanTotalTime ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation
```{r ttbc}
boxcox(meanTotalTime ~ inputType*taskType*block, data=data.long, plotit=T)
```

An effective lambda value is -0.25.

```{r tttransform}
datatr = data.long %>%
    mutate(meanTotalTime = meanTotalTime^(-0.25))

m <- aov(meanTotalTime ~ taskType*inputType*block, data=datatr)
pander(normalCheck(m))

```

### Repeated measures ANOVA on transformed data
```{r ttanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanTotalTime), wid=.(participantNo), within=.(inputType,taskType,block), detailed=TRUE)

# kable(anova$`Mauchly's Test for Sphericity`)
# kable(anova$`Sphericity Corrections`)
# kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
task = anova_apa$text[3]
inter = anova_apa$text[4]
kable(anova_apa)
```

Main effect of block, no interactions with other variables.

### Post-hoc analysis with Tukey HSD

```{r ttph}
t <- TukeyHSD(m)
kable(t$block)
plot(t)
```

# Words per Minute

## Effects of Input Type and Task Type and Block

### Normality Check

```{r wpmnormal}
m <- aov(meanWpm ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation

```{r wpmbc}
boxcox(meanWpm ~ inputType*taskType*block, data=data.long, plotit=T)
```

An effective lambda value is 0.1

```{r wpmtransform}
datatr = data.long %>%
    mutate(meanWpm = meanWpm^(0.1))

m <- aov(meanWpm ~ taskType*inputType*block, data=datatr)
pander(normalCheck(m))
```

### Repeated measures ANOVA on transformed data

```{r wpmanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanWpm), wid=.(participantNo), within=.(inputType,taskType,block), detailed=TRUE)

anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
task = anova_apa$text[3]
inter = anova_apa$text[4]
kable(anova_apa)
```

No effect of block on WPM

# Corrected Error Rate

## Effects of Input Type and Task Type and Block

### Normality Check
```{r}
m <- aov(mean_cer ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed. 

### ANOVAs in ART Data
```{r}
a = art(mean_cer ~ taskType*inputType*block + (1|participantNo), data=data.long)
anova(a)
```

No effect of block.

# Uncorrected Error Rate

## Effects of Input Type and Task Type and Block

### Normality Check
```{r}
m <- aov(mean_uer ~ taskType*inputType*block, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed. 

### ANOVAs in ART Data
```{r}
a = art(mean_uer ~ taskType*inputType*block + (1|participantNo), data=data.long)
anova(a)
```

No effect of block.
