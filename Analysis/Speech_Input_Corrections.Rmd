---
title: "Speech Input Corrections"
author: ""
date: "June 27, 2019"
output: 
   html_document: 
     dev: png
     fig_height: 5
     fig_width: 5.5
     number_sections: yes
     toc: yes
     toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ARTool)
library(reshape2)
library(ez)
library(apa)
library(gridExtra)
library(phia)
library(viridis)
library(lsmeans)
library(gmodels)
library(DescTools)
library(MASS)
library(pander)
library(reticulate)
library(ggpubr)
library(xtable)
library(emmeans)
# Needed to correctly export fonts in pdf (may not be required)
library(extrafont)
# Need to call extrafont::font_import() once in console and restart RStudio

```

```{r functions, echo=FALSE, warning=FALSE}

normalCheck = function(model) {
    res = residuals(model)
    qqnorm((res - mean(res)) / sd(res))
    abline(0, 1)
    print (shapiro.test(res))
}

createPlot = function(data, mean_var, cl_var, cu_var, vj = 3, lx = 0.9, ly = 0.85, y_axis, lim_min = 0, lim_max = 33, breaks = 5, x_axis = "Task") {
  localenv <- environment()
  if (is.element(mean_var, c("mIT", "mTT", "mET", "mPT", "mCT"))) {
    data[mean_var] = data[mean_var] / 1000
    data[cl_var] = data[cl_var] / 1000
    data[cu_var] = data[cu_var] / 1000
  } 
  
  if (mean_var=="m_cer" | mean_var=="m_uer") {
    data[mean_var] = data[mean_var] * 100
    data[cl_var] = data[cl_var] * 100
    data[cu_var] = data[cu_var] * 100
  }
  
  ggplot(data, aes(x=taskType, y=data[[mean_var]], fill=inputType), environment = localenv ) +
  geom_bar(aes(group=inputType), position = position_dodge(.7), colour="black", stat="identity", width=.7) +
  geom_errorbar(aes(ymin = data[[cl_var]], ymax = data[[cu_var]]), width = 0.2, size = .7, position = position_dodge(.7)) +
  geom_text(aes(label=round(data[[mean_var]],digits=2)), size = 4, position = position_dodge(0.7), vjust = vj, alignment="center") +
  scale_x_discrete(name=x_axis) +
  scale_y_continuous(name=y_axis, limits = c(lim_min, lim_max), minor_breaks = breaks, expand = c(0, 0)) +
  #scale_fill_viridis(option="inferno", begin=0.5, end = .8, discrete = TRUE) +
  #scale_fill_viridis(option="magma", begin=0.5, end = .8, discrete = TRUE) +
  scale_fill_manual(values=c("#DF5E28", "#F6C584")) +
  labs(fill = "Input Type") +
  theme(legend.position = c(lx, ly),
        legend.box.background = element_rect(size=1, color="black"),
        legend.background = element_rect(size = 0.3, linetype = "solid", colour = "black"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        axis.title = element_text(family="Helvetica", face="bold", colour="black", size="10"),
        axis.text = element_text(family="Helvetica", colour="black", size="10"),
        panel.grid.major.y = element_line( size = .1, color = "grey"))
}
```

```{python usefulfunctions, echo=FALSE}
import re
def parseAnova(s):
  aovres = re.findall(r'[-+]?\d*\.\d+|\d+',s)
  return "\\anova{%s}{%s}{%s}{%s}{%s}"%(aovres[0], aovres[1], aovres[2], aovres[3], aovres[4])
  
def writeToFile(s, file):
  f = open("autogen/" + file,"w+")
  f.write("% do not edit this file as it was automatically generated\n\n")
  f.write(s)
  f.close()
```

# Data Parsing

## Loading Data

Filtering out invalid participants (9, 10, and 14).

```{r load}
data_c = read.csv("parse_more.csv",sep=",") %>% filter(!participantNo %in% c(9, 10, 14))
data_c = data_c[,!names(data_c) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "correctMsg", "gMsg")]
data_c.kbd = filter(data_c, inputType=="Keyboard")
data_c.sr = filter(data_c, inputType=="Speech")

data_t = read.csv("parse_more_trans.csv",sep=",")
data_t = data_t[,!names(data_t) %in% c("incorrect_fixed", "fixes", "correct", "inf", "msg", "orgMsg")]
data_t$setNum = substr(data_t$setNum, 1, 1)
data_t.kbd = filter(data_t, inputType=="Keyboard")
data_t.sr = filter(data_t, inputType=="Speech")

```

## Outlier Culling

I removed values outside of 3 standard deviations for the means of each task x method combination, for totalTime.

22 outliers identified for composition (2.04% of trials)

```{r outliers_comp, warning=FALSE, message=FALSE, echo=FALSE}
dcs = summarize(group_by(data_c, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

cko.tt <- filter(data_c.kbd, totalTime < (dcs$mTT[1] - 3 * dcs$sdTT[1]) | totalTime > (dcs$mTT[1] + 3 * dcs$sdTT[1]))
cso.tt <- filter(data_c.sr, totalTime < (dcs$mTT[2] - 3 * dcs$sdTT[2]) | totalTime > (dcs$mTT[2] + 3 * dcs$sdTT[2]))
co.tt <- bind_rows(cko.tt, cso.tt)

data_c.clean = anti_join(data_c, co.tt)
#remove(co.all, co.it, co.pt, co.et, co.tt, co.wpm, co.uer, co.cer)
```

26 outliers identified for transcription (2.32% of trials)

```{r outlier_trans, warning=FALSE, message=FALSE, echo=FALSE}
dts = summarize(group_by(data_t, inputType), mIT = mean(inputTime), sdIT = sd(inputTime), mPT = mean(prepTime), sdPT = sd(prepTime), mET = mean(entryTime), sdET=sd(entryTime), mTT = mean(totalTime), sdTT = sd(totalTime), meanWpm = mean(wpm), sdWpm = sd(wpm), mean_uer = mean(uncorrected_error_rate), sd_uer = sd(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), sd_cer = sd(corrected_error_rate), mean_bd = mean(bandwidth))

tko.tt <- filter(data_t.kbd, totalTime < (dts$mTT[1] - 3 * dts$sdTT[1]) | totalTime > (dts$mTT[1] + 3 * dts$sdTT[1]))
tso.tt <- filter(data_t.sr, totalTime < (dts$mTT[2] - 3 * dts$sdTT[2]) | totalTime > (dts$mTT[2] + 3 * dts$sdTT[2]))
to.tt <- bind_rows(tko.tt, tso.tt)
# 
# to.all <- bind_rows(to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)
# to <- unique(to.all)

data_t.clean = anti_join(data_t, to.tt)
#remove(to.all, to.it, to.pt, to.et, to.tt, to.wpm, to.uer, to.cer)

```

## Data Aggregation

Aggregating the cleaned data

```{r warning=FALSE}
data.cr = bind_rows(data_t.clean, data_c.clean) %>% filter(corrTime > 0)
data.ncr = bind_rows(data_t.clean, data_c.clean) %>% filter(corrTime <= 0)
data = bind_rows(data_t.clean, data_c.clean)
data$hadCorrections = ifelse(data$corrTime > 0, TRUE, FALSE)

data.group.cr = summarize(group_by(data.cr, participantNo, inputType, taskType, setNum), meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.group.ncr = summarize(group_by(data.ncr, participantNo, inputType, taskType, setNum), meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.group = summarize(group_by(data, participantNo, inputType, taskType, hadCorrections), meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.stats.corr = data %>%
  group_by(hadCorrections) %>%
  summarize(meanInputTime = mean(inputTime), meanPrepTime = mean(prepTime), meanEntryTime = mean(entryTime), meanTotalTime = mean(totalTime), meanLen = mean(msgLen), meanWpm = mean(wpm), mean_uer = mean(uncorrected_error_rate), mean_cer= mean(corrected_error_rate), mean_bd = mean(bandwidth), meanCorrTime = mean(corrTime))

data.stats.cr = data.cr %>% 
  group_by(inputType, taskType) %>% 
  summarize(mIT = mean(inputTime), cuIT = ci(inputTime)[3], clIT = ci(inputTime)[2], sdIT = sd(inputTime),
            mPT = mean(prepTime), cuPT = ci(prepTime)[3], clPT = ci(prepTime)[2], sdPT = sd(prepTime),
            mET = mean(entryTime), cuET=ci(entryTime)[3], clET = ci(entryTime)[2], sdET = sd(entryTime),
            mTT = mean(totalTime), cuTT = ci(totalTime)[3], clTT = ci(totalTime)[2], sdTT = sd(totalTime),
            mCT = mean(corrTime), cuCT = ci(corrTime)[3], clCT = ci(corrTime)[2], sdCT = sd(corrTime),
            mWpm = mean(wpm), cuWpm = ci(wpm)[3], clWpm = ci(wpm)[2], sdWpm = sd(wpm),
            m_uer = mean(uncorrected_error_rate), cu_uer = ci(uncorrected_error_rate)[3], cl_uer = ci(uncorrected_error_rate)[2], sd_uer = sd(uncorrected_error_rate),
            m_cer= mean(corrected_error_rate), cu_cer = ci(corrected_error_rate)[3], cl_cer = ci(corrected_error_rate)[2], sd_cer = sd(corrected_error_rate)) 

data.stats.ncr = data.ncr %>% 
  group_by(inputType, taskType) %>% 
  summarize(mIT = mean(inputTime), cuIT = ci(inputTime)[3], clIT = ci(inputTime)[2], sdIT = sd(inputTime),
            mPT = mean(prepTime), cuPT = ci(prepTime)[3], clPT = ci(prepTime)[2], sdPT = sd(prepTime),
            mET = mean(entryTime), cuET=ci(entryTime)[3], clET = ci(entryTime)[2], sdET = sd(entryTime),
            mTT = mean(totalTime), cuTT = ci(totalTime)[3], clTT = ci(totalTime)[2], sdTT = sd(totalTime),
            mCT = mean(corrTime), cuCT = ci(corrTime)[3], clCT = ci(corrTime)[2], sdCT = sd(corrTime),
            mWpm = mean(wpm), cuWpm = ci(wpm)[3], clWpm = ci(wpm)[2], sdWpm = sd(wpm),
            m_uer = mean(uncorrected_error_rate), cu_uer = ci(uncorrected_error_rate)[3], cl_uer = ci(uncorrected_error_rate)[2], sd_uer = sd(uncorrected_error_rate),
            m_cer= mean(corrected_error_rate), cu_cer = ci(corrected_error_rate)[3], cl_cer = ci(corrected_error_rate)[2], sd_cer = sd(corrected_error_rate)) 

data.long = melt(data.group, id = c("participantNo", "inputType", "taskType", "hadCorrections", "meanInputTime", "meanPrepTime", "meanWpm", "meanTotalTime", "meanEntryTime", "meanLen", "mean_uer", "mean_cer", "mean_bd", "meanCorrTime"))

data.long$participantNo <- factor(data.long$participantNo)
data.long$inputType <- factor(data.long$inputType)
data.long$hadCorrections<- factor(data.long$hadCorrections)
data.long$taskType <- factor(data.long$taskType)

```


# Input Time

## Chart
```{r}
kable(dplyr::select(data.stats.cr, inputType, taskType, mIT, sdIT))

kable(dplyr::select(data.stats.ncr, inputType, taskType, mIT, sdIT))
```

## Bar Graph

```{r , warning=FALSE, message=FALSE}
createPlot(data.stats.cr, "mIT", "clIT", "cuIT", 3, 0.9, 0.85, "Average Input Time (s)", x_axis="Task (Corrections)")
createPlot(data.stats.ncr, "mIT", "clIT", "cuIT", 3, 0.9, 0.85, "Average Input Time (s)", x_axis="Task (No Corrections)")
```

## Effects of Input Type and Task Type

### Normality Check

```{r itnormal}
m <- aov(meanInputTime ~ hadCorrections, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation
```{r itbc}
boxcox(meanInputTime ~ hadCorrections, data=data.long, plotit=T)
```

 An effective lambda value is 0.1 (a.k.a, a log transformation).

```{r ittransform}
datatr = data.long %>%
    mutate(meanInputTime = meanInputTime^(0.1))

m <- aov(meanInputTime ~ hadCorrections, data=datatr)
pander(normalCheck(m))
```

### Repeated measures ANOVA on transformed data

```{r itanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanInputTime), wid=.(participantNo), within=.(hadCorrections), detailed=TRUE)
# # kable(anova$`Mauchly's Test for Sphericity`)
# # kable(anova$`Sphericity Corrections`)
# # kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
kable(anova_apa)
```

### Post-hoc analysis 

```{r itph}
t <- TukeyHSD(m)
kable(t$`hadCorrections`)
```

```{python}
s = "There was a significant main effect of corrections on Box-Cox transformed \\m{Input Time} (%s)." % (parseAnova(r.input))
writeToFile(s, "corr_inputtime.tex")
```

# Prep Time

## Chart
```{r}
kable(dplyr::select(data.stats.cr, inputType, taskType, mPT, sdPT))

kable(dplyr::select(data.stats.ncr, inputType, taskType, mPT, sdPT))
```

## Bar Graph

```{r warning=FALSE, message=FALSE}
createPlot(data.stats.cr, "mPT", "clPT", "cuPT", 3, 0.9, 0.85, "Average Prep Time (s)", x_axis="Task (Corrections)")
createPlot(data.stats.ncr, "mPT", "clPT", "cuPT", 3, 0.9, 0.85, "Average Prep Time (s)", x_axis="Task (No Corrections)")
```

## Effects of Input Type and Task Type 

### Normality Check

```{r ptnormal}
m <- aov(meanPrepTime ~ hadCorrections, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation

```{r ptbc}
boxcox(meanPrepTime ~ hadCorrections, data=data.long, plotit=T)
```
 
An effective lambda value is -0.1.
 
```{r pttransform}
datatr = data.long %>%
    mutate(meanPrepTime = meanPrepTime^(-0.1))

m <- aov(meanPrepTime ~ hadCorrections, data=datatr)
pander(normalCheck(m))

```
 
### Repeated measures ANOVA on transformed data

```{r ptanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanPrepTime), wid=.(participantNo), within=.(hadCorrections), detailed=TRUE)

# kable(anova$`Mauchly's Test for Sphericity`)
# kable(anova$`Sphericity Corrections`)
# kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
kable(anova_apa)
```

```{python}
s = "There was a significant main effect of corrections on Box-Cox transformed \\m{Prep Time} (%s)." % (parseAnova(r.input))
writeToFile(s, "corr_preptime.tex")
```

# Total Time

## Chart
```{r}
kable(dplyr::select(data.stats.cr, inputType, taskType, mTT, sdTT))

kable(dplyr::select(data.stats.ncr, inputType, taskType, mTT, sdTT))
```

## Bar Graph

```{r , warning=FALSE, message=FALSE}
createPlot(data.stats.cr, "mTT", "clTT", "cuTT", 2.5, 0.9, 0.85, "Average Total Time (s)", 0, 37, 5,x_axis="Task (Corrections)")
createPlot(data.stats.ncr, "mTT", "clTT", "cuTT", 2.5, 0.9, 0.85, "Average Total Time (s)",0, 37, 5, x_axis="Task (No Corrections)")
```

## Effects of Input Type and Task Type

### Normality Check

```{r ttnormal}
m <- aov(meanTotalTime ~ hadCorrections, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation
```{r ttbc}
boxcox(meanTotalTime ~ hadCorrections, data=data.long, plotit=T)
```

An effective lambda value is -0.25.

```{r tttransform}
datatr = data.long %>%
    mutate(meanTotalTime = meanTotalTime^(-0.25))

m <- aov(meanTotalTime ~ hadCorrections, data=datatr)
pander(normalCheck(m))

```

### Repeated measures ANOVA on transformed data
```{r ttanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanTotalTime), wid=.(participantNo), within=.(hadCorrections), detailed=TRUE)

# kable(anova$`Mauchly's Test for Sphericity`)
# kable(anova$`Sphericity Corrections`)
# kable(anova$ANOVA)
anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
kable(anova_apa)
```


### Post-hoc analysis with Tukey HSD

```{r ttph}
t <- TukeyHSD(m)
kable(t$`hadCorrections`)
```

```{python}
s = "There were significant main effect of corrections on Box-Cox transformed \\m{Total Time} (%s)." % (parseAnova(r.input))
writeToFile(s, "corr_totaltime.tex")
```

# Words per Minute

## Chart
```{r}
kable(dplyr::select(data.stats.cr, inputType, taskType, mWpm, sdWpm))
kable(dplyr::select(data.stats.ncr, inputType, taskType, mWpm, sdWpm))
```

## Bar Graph

```{r wpmgraph, warning=FALSE, message=FALSE}
createPlot(data.stats.cr, "mWpm", "clWpm", "cuWpm", 3, .15, .85, "Average WPM", 0, 210, 50, x_axis="Task (Corrections)")
createPlot(data.stats.ncr, "mWpm", "clWpm", "cuWpm", 3, .15, .85, "Average WPM", 0, 210, 50, x_axis="Task (No Corrections)")
```

## Effects of Input Type and Task Type

### Normality Check

```{r wpmnormal}
m <- aov(meanWpm ~ hadCorrections, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### Box-Cox Transformation

```{r wpmbc}
boxcox(meanWpm ~ hadCorrections, data=data.long, plotit=T)
```

An effective lambda value is -0.1

```{r wpmtransform}
datatr = data.long %>%
    mutate(meanWpm = (meanWpm)^(-0.1))

m <- aov(meanWpm ~ hadCorrections, data=datatr)
pander(normalCheck(m))
```

### Repeated measures ANOVA on transformed data

```{r wpmanova, warning=FALSE}
anova = ezANOVA(datatr, dv=.(meanWpm), wid=.(participantNo), within=.(hadCorrections), detailed=TRUE)

anova_apa = anova_apa(anova, sph_corr ="gg", es = "ges", print=FALSE)
input = anova_apa$text[2]
kable(anova_apa)
```

There are significant differences between Keyboard and Speech, and Composition and Transcription, but not the interaction between the two.

```{python}
s = "There was a significant main effect of corrections on Box-Cox transformed \\m{WPM} (%s)." % (parseAnova(r.input))
writeToFile(s, "corr_wpm.tex")
```

### Post-hoc analysis with Tukey HSD

```{r wpmph}
t <- TukeyHSD(m)
kable(t$`hadCorrections`)
```

# Uncorrected Error Rate

## Chart
```{r}
kable(dplyr::select(data.stats.cr, inputType, taskType, m_uer, sd_uer))
kable(dplyr::select(data.stats.ncr, inputType, taskType, m_uer, sd_uer))
```

## Bar Graph
```{r uergraph, warning=FALSE, message=FALSE}
createPlot(data.stats.cr, "m_uer", "cl_uer", "cu_uer", 4, 0.15, 0.85, "Average Uncorrected Error Rate (%)", 0, 2.5, 0.3, x_axis = "Task (Corrections)")
createPlot(data.stats.ncr, "m_uer", "cl_uer", "cu_uer", 4, 0.15, 0.85, "Average Uncorrected Error Rate (%)", 0, 2.5, 0.3, x_axis = "Task (No Corrections)")
```

## Effects of Input Type and Task Type

### Normality Check
```{r}
m <- aov(mean_uer ~ hadCorrections, data=data.long)
pander(normalCheck(m))
```

The data is not normally distributed.

### ANOVAs in ART Data
```{r}
a = art(mean_uer ~ hadCorrections + (1|participantNo), data=data.long)
anova(a)
```

Significant effect of inputType.

### Post-hoc Corrections

```{r}
emmeans(artlm(a, "hadCorrections"), pairwise ~ hadCorrections)
```
